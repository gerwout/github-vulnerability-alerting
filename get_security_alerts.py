import tempfile
import os
import hashlib
import requests
import calendar
import time
import _pickle as pickle
import pprint
import urllib.parse
import json
import xlwt
import argparse
import sys

def print_and_log(msg):
    print(msg)
    Logoutput.log_text = Logoutput.log_text + msg + "\n"

def do_github_api_request_v4(query, method='post', add_headers=[]):
    url = "https://api.github.com/graphql"
    tmp = url + str(hashlib.md5(query.encode('utf-8')).hexdigest()) + method
    cache_name = Settings.tmp_dir + hashlib.md5(tmp.encode('utf-8')).hexdigest() + ".cache"
    print_and_log("Information: cache file: " + cache_name)
    if Settings.force_cache and method == "post" and os.path.isfile(cache_name):
        use_cache = True
    elif Settings.github_use_cache and method == "post" and os.path.isfile(cache_name):
        file_time = os.path.getmtime(cache_name)
        cur_time = calendar.timegm(time.gmtime())
        if cur_time - file_time <= Settings.cache_time_in_seconds:
            use_cache = True
        else:
            use_cache = False
    else:
        use_cache = False

    if not use_cache:
        headers = {'User-Agent': 'github_security_alerts', 'Authorization': "bearer " + Settings.github_auth_token}
        for header in add_headers:
            headers.update(header)
        if method == "post":
            response = requests.post(url, data=json.dumps({'query': query}), headers=headers)
            http_code = response.status_code
            results = response.json()
            if 'errors' in results:
                pprint.pprint(results)
                exit(0)
            pickle.dump(results, open(cache_name, "wb"))
            return results
    else:
        with open(cache_name, "rb") as cache_file:
            results = pickle.load(cache_file)
            return results

def do_github_api_request_v3(url, params={}, method='get', add_headers=[]):
    url = "https://api.github.com" + url
    tmp = url + str(params) + method
    cache_name = Settings.tmp_dir + hashlib.md5(tmp.encode('utf-8')).hexdigest() + ".cache"

    if Settings.force_cache and method == "get" and os.path.isfile(cache_name):
      use_cache = True
    elif Settings.github_use_cache and method == "get" and os.path.isfile(cache_name):
        file_time = os.path.getmtime(cache_name)
        cur_time = calendar.timegm(time.gmtime())
        if cur_time - file_time <= Settings.cache_time_in_seconds:
            use_cache = True
        else:
            use_cache = False
    else:
        use_cache = False

    if not use_cache:
        headers = {'User-Agent': 'github_security_alerts', 'Authorization': "token " + Settings.github_auth_token}
        for header in add_headers:
            headers.update(header)

        if method == 'get':
            response = requests.get(url, params=params, headers=headers)
            http_code = response.status_code
            if http_code == 200:
                results = response.json()
                pickle.dump(results, open(cache_name, "wb"))
                return results
            elif http_code == 403:
                rate_limit_remaining = response.headers.get('x-ratelimit-remaining', False)
                if rate_limit_remaining == '0':
                    raise Exception('The given token has been rate limited, please retry in an hour')
                return response.status_code, False, response.json()
            # HTTP no content, is considered valid in some of the Github calls
            elif http_code == 204 or http_code == 404:
                return response.status_code, False
            else:
                # print(response.json())
                # print(response.headers)
                return response.status_code, False, response.json()
        elif method == 'put':
            response = requests.put(url, headers=headers)
            try:
                return response.json()
            except:
                return response.status_code, False
        elif method == 'delete':
            response = requests.delete(url, headers=headers)
            try:
                return response.json()
            except:
                return response.status_code, False
    else:
        with open(cache_name, "rb") as cache_file:
            results = pickle.load(cache_file)
            return results

# needs admin:org read:org
def get_all_orgs():
    orgs = do_github_api_request_v3('/user/orgs')

    return orgs

# needs  (:read, :write, or :admin)
def get_all_repos_from_org(org, page=1):
    url = "/orgs/" + urllib.parse.quote(org) + "/repos?per_page=100&page=" + str(page)
    repos = do_github_api_request_v3(url)
    # we need to paginate
    if (len(repos) == 100):
        repos = repos + get_all_repos_from_org(org, page=page+1)
        return repos
    else:
        return repos

# needs :admin
def are_vulnerability_alerts_enabled(repo, owner):
    headers = {'Accept': 'application/vnd.github.dorian-preview+json'}
    url = "/repos/" + urllib.parse.quote(owner) + "/" + urllib.parse.quote(repo) + "/vulnerability-alerts"
    response = do_github_api_request_v3(url, add_headers=[headers])
    if response[0] == 204:
        return True
    else:
        return False

# needs :admin:org write:org
def enable_security_alerts(repo, owner):
    headers = {'Accept': 'application/vnd.github.dorian-preview+json'}
    url = "/repos/" + urllib.parse.quote(owner) + "/" + urllib.parse.quote(repo) + "/vulnerability-alerts"
    response = do_github_api_request_v3(url, add_headers=[headers], method='put')

    return response

def get_vulnerability_alerts(repo, owner):
    headers = {'Accept': 'application/vnd.github.vixen-preview+json'}

    graphql_query = '''{{
          repository(name: "{repo}", owner: "{owner}") {{
            vulnerabilityAlerts(first: 100) {{
              nodes {{
                dismissReason
                dismissedAt
                dismisser {{ 
                    id
                    email
                    login
                    name
                    url
                    websiteUrl 
                }}
                securityAdvisory {{ 
                    description
                    identifiers {{
                        type
                        value 
                    }}
                    origin
                    publishedAt
                    references {{
                        url   
                    }}
                    severity
                    summary
                    updatedAt
                    withdrawnAt
                }}
                securityVulnerability {{
                    firstPatchedVersion {{ 
                        identifier
                    }}
                    package {{
                        ecosystem
                        name
                    }}
                    severity
                    updatedAt                
                    vulnerableVersionRange 
                }}
                vulnerableRequirements
              }}
            }}
          }}
        }}'''.format(owner=urllib.parse.quote(owner), repo=urllib.parse.quote(repo))

    response = do_github_api_request_v4(graphql_query, add_headers=[headers])

    return response

def get_value_as_string(value):
    if value is None:
        value = "-"
    elif not isinstance(value, str):
        value = json.dumps(value, indent=4)

    return value

def createExcelSheet(list):
    excel_doc = xlwt.Workbook()
    sheet = excel_doc.add_sheet('Known vulnerabilities')
    sheet2 = excel_doc.add_sheet('Information')
    logs_list = Logoutput.log_text.split("\n")
    for index, val in enumerate(logs_list):
        sheet2.col(0).width = 256 * 100
        sheet2.write(index, 0, val)

    style = xlwt.XFStyle()
    style.alignment.wrap = 1
    sheet.write(0, 0, "Repository")
    sheet.write(0, 1, "Count")
    sheet.write(0, 2, "Vulnerability details")
    sheet.write(0, 3, "Full details")
    for index, val in enumerate(list):
        repo = val['repo']
        count = val['count']
        summary = val['summary']
        vulnerabilities = val['vulnerabilities']

        if len(vulnerabilities) > 32676:
            vulnerabilities = vulnerabilities[:32676]
        sheet.col(0).width = 256 * 30
        sheet.col(1).width = 256 * 10
        sheet.col(2).width = 256 * 100
        sheet.col(3).width = 256 * 100

        sheet.write(index + 1, 0, repo)
        sheet.write(index + 1, 1, count)
        sheet.write(index + 1, 2, summary, style)
        sheet.write(index + 1, 3, vulnerabilities, style)
    print_and_log("Information: saving " + Settings.file_name)
    try:
        excel_doc.save(Settings.file_name)
    except:
        print_and_log("Error: Could not write excel file " + Settings.file_name + ", is it still open in Excel?")

class Settings:
    file_name = ''
    github_auth_token = ''
    github_use_cache = True
    force_cache = False
    cache_time_in_seconds = 3600
    tmp_dir = ''

class Logoutput:
    log_text = ''


def main():
    parser = argparse.ArgumentParser(description='Get all known security vulnerabilities from Github (for all organisations)')
    parser.add_argument('--file', '-f', required=True, help='Excel file name (xls extension)')
    parser.add_argument('--authtoken', '-a', required=True, help='Github personal access token')
    parser.add_argument('--disablecache', '-dc', required=False, default=False, type=bool, help='Disable Github cached results')
    parser.add_argument('--forcecache', '-fc', required=False, default=False, type=bool, help='Force usage of Github cached results (i.e. ignore cache time)')
    parser.add_argument('--cachetime', '-ct', required=False, default=3600, type=int, help='Cache timeout in seconds, default = 3600')

    # show help when no arguments are given
    if len(sys.argv) == 1:
        parser.print_help(sys.stdout)
        sys.exit(0)

    args = parser.parse_args()
    Settings.file_name = args.file
    Settings.github_auth_token = args.authtoken
    Settings.cache_time_in_seconds = args.cachetime
    force_cache = args.forcecache
    disable_cache = args.disablecache
    if force_cache and disable_cache:
        parser.print_help(sys.stdout)
        print("You can't combine disabling and forcing the cache")
        sys.exit(0)
    Settings.github_use_cache = not disable_cache
    Settings.force_cache = force_cache

    if (os.name == 'nt'):
        path_seperator = "\\"
    else:
        path_seperator = "/"
    Settings.tmp_dir = tempfile.gettempdir() + path_seperator

    print_and_log("Information: using " + Settings.tmp_dir + " for cache.")

    orgs = get_all_orgs()
    repo_vulns = {}

    for org in orgs:
        name = org['login']
        repos = get_all_repos_from_org(name)
        for repo in repos:
            # bool
            archived = repo['archived']
            clone_url = repo['clone_url']
            git_url = repo['git_url']
            ssh_url = repo['ssh_url']
            is_fork = repo['fork']
            repo_name = repo['name']
            full_name = repo['full_name']
            owner = repo['owner']['login']
            private = repo['private']
            if not private:
                print_and_log("Warning: code base " + full_name + " is not a private repository!")
            if not archived and not is_fork:
                vulnerability_alerts = are_vulnerability_alerts_enabled(repo_name, owner)
                if not vulnerability_alerts:
                    print_and_log("Information: vulnerability alerts disabled for code base: " + clone_url)
                    print_and_log("Information: enabling vulnerability alerts for code base: " + clone_url)
                    result = enable_security_alerts(repo_name, owner)
                    if not result:
                        print_and_log("Error: failed to enable vulnerability alerts for code base: " + clone_url)
                print_and_log("Information: fetching vulnerabilities for " + full_name)
                repo_vulns[full_name] = get_vulnerability_alerts(repo_name, owner)
            else:
                if archived:
                    print_and_log("Information: ignoring archived code base: " + clone_url)
                elif is_fork:
                    print_and_log("Information: ignoring forked code base: " + clone_url)

    overview = []
    keys = repo_vulns.keys()

    for key in keys:
        vulns = repo_vulns[key]
        if vulns is None:
            print_and_log("Information: code base " + key + " has no known vulnerable dependencies.")
        else:
            nodes = vulns['data']['repository']['vulnerabilityAlerts']['nodes']
            temp_dict = {"repo": key, "count": len(nodes) }
            vuln_list = []
            short_vuln_list = []

            for vulnerability in nodes:
                level = vulnerability['securityAdvisory']["severity"]
                summary = vulnerability['securityAdvisory']["summary"]
                ref = vulnerability['securityAdvisory']["references"][0]['url']
                temp = level + " - " + summary + " - " + ref
                vuln_list.append(vulnerability)
                short_vuln_list.append(temp)

            temp_dict["vulnerabilities"] = get_value_as_string(vuln_list)
            temp_dict["summary"] = get_value_as_string(short_vuln_list)
            overview.append(temp_dict)

    createExcelSheet(overview)
    print_and_log("Information: created vulnerability report!")
    exit(0)

if __name__ == "__main__":
    main()